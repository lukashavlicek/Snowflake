-- 1) USER STAGE @~
-- Putting data into a user-stage
-- SnowSql: PUT file://C:\Snowflake\load_temp\TDFF_Riders_History*csv @~/stg01;
-- PUT command automatically compresses the file

list @~/stg01;

SELECT 
$3 as Rider
FROM @~/stg01 (file_format => 'MyCsvForStages');

COPY INTO tdf_rider
FROM
(SELECT 
$3 as Rider
FROM @~/stg01 (file_format => 'MyCsvForStages'));

-- purge = true -- deleted the file from user-stage after copying
----------------------------------------------
-- 2) TABLE STAGE @%
-- table stage is crated for every snowflake table
-- putting .csv "Ocean" sample file generated by GPT into a table stage, then loading it into the table itself
-- Using file_format MyCsvForStages that has been already created

-- Creating the table first

CREATE OR REPLACE TABLE Oceans (
Ocean string,
Area number,
AverageDepth number,
DeepestPoint number
)

SELECT * FROM Oceans;

-- Putting data into a table-stage of the Oceans table
-- SnowSql: PUT file://C:\Snowflake\load_temp\Oceans*csv @%Oceans;

list @%Oceans;
--remove @%Oceans;
select * from @%Oceans;

-- copy data from table stage into a table, specify the file_format to skip the first row which is a header
-- stage does not need to be specified, it is automatically copied from table stage into a table
copy into Oceans
file_format = (format_name = MyCsvForStages);

SELECT * FROM Oceans ORDER BY Area DESC;

---------------------------------------
-- Named intrnal stage (using same csv dataset)

CREATE TABLE Oceans_clone CLONE Oceans;

CREATE STAGE Ocean_stage;

-- Putting data into a named-stage of the Oceans_clone table
-- SnowSql: PUT file://C:\Snowflake\load_temp\Oceans*csv @Ocean_stage;


list @Ocean_stage;
show stages;

-- Query data from the stage
SELECT 
$1 as Name,
$2,
$3,
$4
FROM @Ocean_stage;

-- copy data from named stage into cloned table
COPY INTO Oceans_clone
FROM
(SELECT 
$1,
$2,
$3,
$4
FROM @Ocean_stage (file_format => 'MyCsvForStages'));

-- Query data from destination table
SELECT * FROM Oceans_clone;

---------------------------------
-- Loading JSON data
-- Associate file format with a table !! USEFUL !!
-- JSON data is displayed automatically as a result when querying the table stage @%

CREATE TABLE Oceans_json (
 json_data variant
)
stage_file_format = (type = 'JSON');

list @%Oceans_json;

-- Putting data into a table stage of the Oceans_json table
-- SnowSql: PUT file://C:\Snowflake\load_temp\Oceans*json @%Oceans_json;

select * from Oceans_json;
-- You can query table stage as JSON file format right away
select * from @%Oceans_json;

-- copy data from table stage into table
copy into Oceans_json;

select * from Oceans_json;

-- Flatten loaded json
select 
arr.value:Name::string as Name
--arr.value:Area (sq. km)::number as Area,
--arr.value:Average Depth (m)::number as Average_depth,
--arr.value:Deepest Point (m)::number as Deepest_point
from Oceans_json as O,
lateral flatten ( input => json_data) arr;

------------------------------------------------
-- Loading multiplce CSV files at one go
-- Using pattern

-- Putting data into a user-stage
-- SnowSql: PUT file://C:\Snowflake\load_temp\Oceans_*.csv @~/csv/uncompressed;
-- Loads Oceans_1.csv and Oceans_2.csv files

list @~/csv/uncompressed;

CREATE OR REPLACE TABLE Oceans_multiple (
Ocean string,
Area number,
AverageDepth number,
DeepestPoint number
)

copy into Oceans_multiple
from @~/csv/uncompressed/
file_format = MyCsvForStages
on_error = 'CONTINUE'
pattern = '.*[.]csv'
purge = TRUE;

-- includes duplicate data from two csv files
select * from Oceans_multiple;

